{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import src.mvae.mt.mvae.utils as utils\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "from functools import partial\n",
    "from geomstats.geometry.hypersphere import Hypersphere\n",
    "from geomstats.geometry.euclidean import Euclidean\n",
    "from geomstats.geometry.hyperbolic import Hyperbolic\n",
    "from geomstats.geometry.product_manifold import ProductManifold\n",
    "from scipy import stats\n",
    "from scipy.io import mmread\n",
    "from src.lightning.gene import GeneModule\n",
    "from src.mvae.mt.data import GeneDataset\n",
    "from src.mvae.mt.mvae.components import *\n",
    "from src.mvae.mt.mvae.distributions import *\n",
    "from src.mvae.mt.mvae.models.gene_vae import GeneVAE\n",
    "from src.mvae.mt.mvae.ops.hyperbolics import lorentz_to_poincare\n",
    "from src.mvae.mt.mvae.ops.spherical import spherical_to_projected\n",
    "from src.rkmeans.rkmeans import RiemannianKMeans, merge_clusters\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "config_path = \"/home/romainlhardy/code/hyperbolic-cancer/configs/cellxgene/cellxgene_e20h20s20.yaml\"\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "dataset = GeneDataset(**config[\"data\"][\"options\"])\n",
    "print(dataset.n_gene_r)\n",
    "print(dataset.n_gene_p)\n",
    "print(dataset.n_batch)\n",
    "print(len(dataset))\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=2048, num_workers=16, shuffle=True)\n",
    "\n",
    "x_r, x_p, batch_idx = dataset[np.random.choice(len(dataset))]\n",
    "print(x_r, x_p, batch_idx)\n",
    "print(x_r.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MC-VAE model\n",
    "checkpoint_path = \"/home/romainlhardy/code/hyperbolic-cancer/models/mvae/cellxgene_mvae_e20h20s20.ckpt\"\n",
    "\n",
    "device = \"cuda\"\n",
    "config[\"lightning\"][\"model\"][\"options\"][\"n_gene_r\"] = dataset.n_gene_r\n",
    "config[\"lightning\"][\"model\"][\"options\"][\"n_gene_p\"] = dataset.n_gene_p\n",
    "config[\"lightning\"][\"model\"][\"options\"][\"n_batch\"] = dataset.n_batch\n",
    "module = GeneModule(config).to(device)\n",
    "\n",
    "if checkpoint_path is not None:\n",
    "    module.load_state_dict(torch.load(checkpoint_path)[\"state_dict\"])\n",
    "\n",
    "model = module.model\n",
    "model.eval()\n",
    "\n",
    "x_r, x_p, batch_idx = next(iter(dataloader))\n",
    "outputs = model(x_r.to(device), x_p.to(device), batch_idx.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute latent embeddings\n",
    "def get_latents(reparametrized, num_components=1):\n",
    "    assert len(reparametrized) > 0\n",
    "\n",
    "    latents = [[] for _ in range(num_components)]\n",
    "    for r in reparametrized:\n",
    "        for i, rr in enumerate(r):\n",
    "            latents[i].append(rr.q_z.loc.detach().cpu().numpy())\n",
    "\n",
    "    for i in range(num_components):\n",
    "        latents[i] = np.concatenate(latents[i], axis=0)\n",
    "        \n",
    "    return latents\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=2048, num_workers=16, shuffle=False)\n",
    "\n",
    "reparametrized = []\n",
    "for batch in tqdm(dataloader):\n",
    "    x_r, x_p, batch_idx = batch\n",
    "    with torch.no_grad():\n",
    "        outputs = model(x_r.to(device), x_p.to(device), batch_idx.to(device))\n",
    "    reparametrized.append(outputs[\"reparametrized\"])\n",
    "\n",
    "num_components = len(model.components)\n",
    "latents = get_latents(reparametrized, num_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a product manifold\n",
    "components = []\n",
    "for component in model.components:\n",
    "    if isinstance(component, EuclideanComponent):\n",
    "        components.append(Euclidean(dim=component.dim))\n",
    "    elif isinstance(component, HyperbolicComponent):\n",
    "        components.append(Hyperbolic(dim=component.dim - 1))\n",
    "    elif isinstance(component, SphericalComponent):\n",
    "        components.append(Hypersphere(dim=component.dim - 1))\n",
    "\n",
    "manifold = ProductManifold(components)\n",
    "print(manifold.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run fine-grained k-means clustering\n",
    "X = np.concatenate(latents, axis=-1)\n",
    "assert X.shape[1] == manifold.shape[0]\n",
    "\n",
    "# Subsample the data if X is large\n",
    "X_sub = X.copy()\n",
    "max_size = 50000\n",
    "if X.shape[0] > max_size:\n",
    "    X_sub = X[np.random.choice(X.shape[0], max_size, replace=False)]\n",
    "\n",
    "rkmeans = RiemannianKMeans(\n",
    "    space=manifold,\n",
    "    n_clusters=1000,\n",
    "    batch_size=len(X_sub),        \n",
    "    random_state=42,\n",
    "    init=\"random\",    \n",
    "    tol=1e-2,\n",
    "    max_iter=1000,\n",
    "    verbose=1,\n",
    ")\n",
    "rkmeans.fit(X_sub)\n",
    "\n",
    "labels = rkmeans.labels_\n",
    "cluster_centers = rkmeans.cluster_centers_\n",
    "print(labels.shape, cluster_centers.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the whole dataset\n",
    "labels = rkmeans.predict(X)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge clusters\n",
    "final_assignments, n_final_clusters = merge_clusters(\n",
    "    manifold, \n",
    "    labels,\n",
    "    cluster_centers,\n",
    "    merge_threshold=1.5,\n",
    "    verbose=1,\n",
    ")\n",
    "print(np.unique(final_assignments, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge small clusters\n",
    "unique_labels, counts = np.unique(final_assignments, return_counts=True)\n",
    "print(f\"Cluster counts before merging small clusters: {counts}\")\n",
    "print(f\"Number of clusters before merging: {len(unique_labels)}\")\n",
    "\n",
    "min_cluster_size = 1000\n",
    "\n",
    "# Identify the largest cluster\n",
    "if len(counts) > 0:\n",
    "    largest_cluster_index = np.argmax(counts)\n",
    "    largest_cluster_label = unique_labels[largest_cluster_index]\n",
    "    print(f\"Largest cluster label: {largest_cluster_label} with size {counts[largest_cluster_index]}\")\n",
    "\n",
    "    # Identify small clusters\n",
    "    small_cluster_indices = np.where(counts < min_cluster_size)[0]\n",
    "    small_cluster_labels = unique_labels[small_cluster_indices]\n",
    "    print(f\"Labels of small clusters (< {min_cluster_size}): {small_cluster_labels}\")\n",
    "\n",
    "    # Reassign elements from small clusters to the largest cluster\n",
    "    # Avoid reassigning the largest cluster to itself if it happens to be small (edge case)\n",
    "    for label in small_cluster_labels:\n",
    "        if label != largest_cluster_label:\n",
    "            final_assignments[final_assignments == label] = largest_cluster_label\n",
    "            print(f\"Reassigned cluster {label} to {largest_cluster_label}\")\n",
    "else:\n",
    "    print(\"No clusters found in final_assignments.\")\n",
    "\n",
    "# Verify the result\n",
    "unique_labels_after, counts_after = np.unique(final_assignments, return_counts=True)\n",
    "print(f\"Final cluster counts after merging: {counts_after}\")\n",
    "print(f\"Number of final clusters after merging: {len(unique_labels_after)}\")\n",
    "print(np.unique(final_assignments, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remap cluster labels to be contiguous from 0\n",
    "unique_labels_final = np.unique(final_assignments)\n",
    "print(f\"Unique labels before remapping: {unique_labels_final}\")\n",
    "\n",
    "# Create a mapping from the current labels to new labels (0, 1, 2, ...)\n",
    "label_mapping = {old_label: new_label for new_label, old_label in enumerate(unique_labels_final)}\n",
    "\n",
    "# Apply the mapping\n",
    "final_assignments = np.array([label_mapping[label] for label in final_assignments])\n",
    "\n",
    "# Verify the remapping\n",
    "unique_labels_remapped, counts_remapped = np.unique(final_assignments, return_counts=True)\n",
    "print(f\"Unique labels after remapping: {unique_labels_remapped}\")\n",
    "print(f\"Counts after remapping: {counts_remapped}\")\n",
    "print(f\"Number of clusters after remapping: {len(unique_labels_remapped)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/home/romainlhardy/code/hyperbolic-cancer/data/cellxgene/climb_cluster_assignments.npy\", final_assignments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
