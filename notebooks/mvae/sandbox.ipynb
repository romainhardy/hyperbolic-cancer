{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# sys.path.append(\"/home/roh3635/hyperbolic-cancer/mvae/mt/mvae\")\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.special\n",
    "import torch\n",
    "import torch.distributions as distributions\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln_2: torch.Tensor = math.log(2)\n",
    "ln_pi: torch.Tensor = math.log(math.pi)\n",
    "ln_2pi: torch.Tensor = ln_2 + ln_pi\n",
    "\n",
    "\n",
    "class EuclideanNormal(distributions.Normal):\n",
    "    def log_prob(self, value):\n",
    "        return super().log_prob(value).sum(dim=-1)\n",
    "\n",
    "\n",
    "class EuclideanUniform(distributions.Uniform):\n",
    "    def log_prob(self, value):\n",
    "        return super().log_prob(value).sum(dim=-1)\n",
    "    \n",
    "\n",
    "class IveFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, v, z):\n",
    "        ctx.save_for_backward(z)\n",
    "        ctx.v = v\n",
    "        z_cpu = z.double().detach().cpu().numpy()\n",
    "        if np.isclose(v, 0):\n",
    "            output = scipy.special.i0e(z_cpu, dtype=z_cpu.dtype)\n",
    "        elif np.isclose(v, 1):\n",
    "            output = scipy.special.i1e(z_cpu, dtype=z_cpu.dtype)\n",
    "        else:\n",
    "            output = scipy.special.ive(v, z_cpu, dtype=z_cpu.dtype)\n",
    "        return torch.tensor(output, dtype=z.dtype, device=z.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        z = ctx.saved_tensors[-1]\n",
    "        return None, grad_output * (ive(ctx.v - 1, z) - ive(ctx.v, z) * (ctx.v + z) / z)\n",
    "\n",
    "\n",
    "def ive(v, z):\n",
    "    return IveFunction.apply(v, z)\n",
    "\n",
    "\n",
    "class HypersphericalUniform(distributions.Distribution):\n",
    "    arg_constraints = {}\n",
    "    support = distributions.constraints.real\n",
    "    _mean_carrier_measure = 0\n",
    "\n",
    "    def __init__(self, dim, validate_args, device=\"cuda\"):\n",
    "        super().__init__(torch.Size([dim]), validate_args=validate_args)\n",
    "        self.dim = dim\n",
    "        self.device = device\n",
    "        self.normal = EuclideanNormal(0, 1)\n",
    "\n",
    "    def rsample(self, sample_shape):\n",
    "        output = self.normal.sample(sample_shape + torch.Size([1, self.dim + 1])).to(self.device)\n",
    "        return F.normalize(output, dim=-1)\n",
    "\n",
    "    def entropy(self):\n",
    "        return self.__log_surface_area()\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        return -torch.ones(x.shape[:-1], device=self.device) * self.__log_surface_area()\n",
    "\n",
    "    def __log_surface_area(self):\n",
    "        n = self.dim\n",
    "        t = torch.tensor((n + 1.) / 2.)\n",
    "        ret = ln_2 + t * ln_pi - torch.lgamma(t)\n",
    "        return ret\n",
    "\n",
    "\n",
    "# class VonMisesFisher(distributions.Distribution):\n",
    "#     arg_constraints = {\n",
    "#         \"loc\": torch.distributions.constraints.real,\n",
    "#         \"scale\": torch.distributions.constraints.positive\n",
    "#     }\n",
    "#     support = torch.distributions.constraints.real\n",
    "#     _mean_carrier_measure = 0\n",
    "\n",
    "#     @property\n",
    "#     def mean(self):\n",
    "#         return self.loc * (ive(self.p / 2, self.scale) / ive(self.p / 2 - 1, self.scale))\n",
    "\n",
    "#     @property\n",
    "#     def stddev(self):\n",
    "#         return self.scale\n",
    "    \n",
    "#     def __init__(self, loc, scale, validate_args=None, device=\"cuda\"):\n",
    "#         self.dtype = loc.dtype\n",
    "#         self.loc = loc\n",
    "#         assert loc.norm(p=2, dim=-1).allclose(torch.ones(loc.shape[:-1], device=loc.device))\n",
    "#         self.scale = scale\n",
    "#         assert (scale > 0).all()\n",
    "#         self.device = loc.device\n",
    "#         self.p = loc.shape[-1]\n",
    "\n",
    "#         self.uniform = EuclideanUniform(0, 1)\n",
    "#         self.hyperspherical_uniform_v = HypersphericalUniform(self.p - 2, device=self.device)\n",
    "\n",
    "#         # Pre-compute Householder transformation\n",
    "#         e1 = torch.tensor([1.] + [0.] * (loc.shape[-1] - 1), requires_grad=False, device=self.device)\n",
    "#         self.u = F.normalize(e1 - self.loc)\n",
    "\n",
    "#         super().__init__(self.loc.size(), validate_args=validate_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The von-Mises-Fisher distribution in PyTorch\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.distributions as distributions\n",
    "\n",
    "from torch.distributions import constraints\n",
    "from torch.distributions.distribution import Distribution\n",
    "\n",
    "\n",
    "class VonMisesFisher(Distribution):\n",
    "    arg_constraints = {\n",
    "        \"loc\": constraints.real_vector,\n",
    "        \"scale\": constraints.positive\n",
    "    }\n",
    "    support = constraints.real_vector\n",
    "    has_rsample = True\n",
    "    \n",
    "    def __init__(self, loc, scale, validate_args=None):\n",
    "        self.dtype = loc.dtype\n",
    "        self.device = loc.device\n",
    "        \n",
    "        if validate_args:\n",
    "            loc_norm = torch.norm(loc, dim=-1)\n",
    "            if not torch.allclose(loc_norm, torch.ones_like(loc_norm), atol=1e-5):\n",
    "                raise ValueError(\"`loc` must be normalized to unit length\")\n",
    "            if not torch.all(scale > 0):\n",
    "                raise ValueError(\"`scale` must be positive\")\n",
    "                \n",
    "        self.loc = loc\n",
    "        self.scale = scale\n",
    "        \n",
    "        self.__m = loc.shape[-1]\n",
    "        self.__e1 = torch.zeros(self.__m, device=self.device, dtype=self.dtype) # Reference vector\n",
    "        self.__e1[0] = 1.0\n",
    "        \n",
    "        batch_shape = torch.broadcast_shapes(loc.shape[:-1], scale.shape)\n",
    "        event_shape = torch.Size([self.__m])\n",
    "        \n",
    "        super(VonMisesFisher, self).__init__(batch_shape, event_shape, validate_args)\n",
    "        \n",
    "    def rsample(self, sample_shape=torch.Size()):\n",
    "        shape = sample_shape + self.batch_shape\n",
    "        \n",
    "        if self.__m == 3:\n",
    "            w = self._sample_w3(shape)\n",
    "        else:\n",
    "            w = self._sample_w_rej(shape)\n",
    "            \n",
    "        w = torch.clamp(w, -1 + 1e-6, 1 - 1e-6)\n",
    "        \n",
    "        v_shape = shape + torch.Size([self.__m - 1])\n",
    "        v = torch.randn(v_shape, device=self.device, dtype=self.dtype)\n",
    "        v = torch.nn.functional.normalize(v, dim=-1)\n",
    "        \n",
    "        # Combine to get a point on the sphere\n",
    "        tmp = torch.sqrt(1.0 + w) * torch.sqrt(1.0 - w)\n",
    "        x = torch.cat([w, tmp.unsqueeze(-1) * v], dim=-1)\n",
    "        \n",
    "        # Apply Householder rotation\n",
    "        z = self._householder_rotation(x)\n",
    "        return z\n",
    "        \n",
    "    def _sample_w3(self, shape):\n",
    "        w_shape = shape + torch.Size([1])\n",
    "        u = torch.rand(w_shape, device=self.device, dtype=self.dtype)\n",
    "        u = torch.clamp(u, 1e-16, 1 - 1e-16)\n",
    "        \n",
    "        w = 1 + (torch.log(u) + torch.log(1 - u) - 2 * self.scale) / self.scale\n",
    "        return w\n",
    "        \n",
    "    def _sample_w_rej(self, shape):\n",
    "        w_shape = shape + torch.Size([1])\n",
    "        m = float(self.__m)\n",
    "        \n",
    "        # Compute b\n",
    "        tmp = torch.sqrt((4 * (self.scale ** 2)) + ((m - 1) ** 2))\n",
    "        b = (m - 1.0) / (2.0 * self.scale + tmp)\n",
    "        \n",
    "        # Prepare for rejection sampling\n",
    "        b_shape = shape + torch.Size([1])\n",
    "        b = b.expand(b_shape)\n",
    "        \n",
    "        w = torch.zeros_like(b)\n",
    "        e = torch.zeros_like(b)\n",
    "        mask = torch.ones_like(b, dtype=torch.bool)\n",
    "        \n",
    "        # Rejection sampling loop - in PyTorch we must do this explicitly\n",
    "        max_attempts = 100\n",
    "        for _ in range(max_attempts):\n",
    "            if not torch.any(mask):\n",
    "                break\n",
    "                \n",
    "            # Sample from Beta distribution\n",
    "            beta_dist = distributions.Beta((m - 1.0) / 2.0, (m - 1.0) / 2.0)\n",
    "            e_new = beta_dist.sample(w_shape).to(self.device).type(self.dtype)\n",
    "            \n",
    "            # Sample uniform\n",
    "            u = torch.rand(w_shape, device=self.device, dtype=self.dtype)\n",
    "            \n",
    "            # Compute w\n",
    "            w_new = (1.0 - (1.0 + b) * e_new) / (1.0 - (1.0 - b) * e_new)\n",
    "            x = (1.0 - b) / (1.0 + b)\n",
    "            c = self.scale * x + (m - 1) * torch.log1p(-x**2)\n",
    "            \n",
    "            # Acceptance test\n",
    "            tmp = torch.clamp(x * w_new, 0, 1 - 1e-16)\n",
    "            log_accept = ((m - 1.0) * torch.log(1.0 - tmp) + self.scale * w_new - c) - torch.log(u)\n",
    "            accept = log_accept > 0\n",
    "            \n",
    "            # Update values\n",
    "            active_mask = mask & accept\n",
    "            w = torch.where(active_mask, w_new, w)\n",
    "            e = torch.where(active_mask, e_new, e)\n",
    "            mask = mask & ~accept\n",
    "            \n",
    "        return w\n",
    "        \n",
    "    def _householder_rotation(self, x):\n",
    "        u = torch.nn.functional.normalize(self.__e1 - self.loc, dim=-1)\n",
    "        z = x - 2 * (u * x).sum(dim=-1, keepdim=True) * u\n",
    "        return z\n",
    "        \n",
    "    def log_prob(self, x):\n",
    "        return self._log_unnormalized_prob(x) - self._log_normalization()\n",
    "        \n",
    "    def _log_unnormalized_prob(self, x):\n",
    "        if self.validate_args:\n",
    "            x_norm = torch.norm(x, dim=-1)\n",
    "            if not torch.allclose(x_norm, torch.ones_like(x_norm), atol=1e-3):\n",
    "                raise ValueError(\"x must be normalized to unit length\")\n",
    "                \n",
    "        return self.scale * torch.sum(self.loc * x, dim=-1)\n",
    "        \n",
    "    def _log_normalization(self):\n",
    "        m = float(self.__m)\n",
    "        return ((m / 2.0 - 1) * torch.log(self.scale) - \n",
    "                (m / 2.0) * math.log(2 * math.pi) - \n",
    "                (self.scale + torch.log(self._ive(m / 2.0 - 1, self.scale))))\n",
    "                \n",
    "    def entropy(self):\n",
    "        m = float(self.__m)\n",
    "        return (- self.scale * self._ive(m / 2.0, self.scale) / \n",
    "                self._ive(m / 2.0 - 1, self.scale) - \n",
    "                self._log_normalization())\n",
    "                \n",
    "    def mean(self):\n",
    "        m = float(self.__m)\n",
    "        return self.loc * (self._ive(m / 2.0, self.scale) / \n",
    "                          self._ive(m / 2.0 - 1, self.scale))\n",
    "                          \n",
    "    def mode(self):\n",
    "        return self.mean()\n",
    "        \n",
    "    def _ive(self, v, z):\n",
    "        \"\"\"\n",
    "        Modified Bessel function of the first kind exp(-|z|) I_v(z)\n",
    "        \n",
    "        Note: This is a simplified implementation. For a production system, \n",
    "        you might want to use a more efficient computation of the Bessel function.\n",
    "        \"\"\"\n",
    "        # This is a placeholder implementation\n",
    "        # In a real system, you would use a specialized numerical implementation\n",
    "        # For example, you could use scipy.special.ive through torch.numpy_interop\n",
    "        # or implement a more efficient version directly in PyTorch\n",
    "        \n",
    "        # For now, returning a simple approximation for small z\n",
    "        # This will need to be replaced with a proper implementation\n",
    "        return torch.exp(-torch.abs(z)) * (z/2)**v / torch.exp(torch.lgamma(v + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "import math\n",
    "\n",
    "class MixedCurvatureVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_gene, \n",
    "        n_batch=None, \n",
    "        z_dim=2,\n",
    "        encoder_layer=None, \n",
    "        decoder_layer=None, \n",
    "        latent_dist=\"vmf\", \n",
    "        observation_dist=\"nb\",\n",
    "        batch_invariant=False\n",
    "    ):\n",
    "        super(MixedCurvatureVAE, self).__init__()\n",
    "        \n",
    "        if encoder_layer is None:\n",
    "            encoder_layer = [128, 64, 32]\n",
    "        if decoder_layer is None:\n",
    "            decoder_layer = [32, 128]\n",
    "        \n",
    "        self.batch_invariant = batch_invariant\n",
    "        self.n_input_feature = n_gene\n",
    "        self.z_dim = z_dim\n",
    "        self.encoder_layer = encoder_layer\n",
    "        self.decoder_layer = decoder_layer\n",
    "        self.latent_dist = latent_dist\n",
    "        self.observation_dist = observation_dist\n",
    "        \n",
    "        if self.latent_dist == \"vmf\":\n",
    "            self.z_dim += 1\n",
    "        \n",
    "        if not isinstance(n_batch, list):\n",
    "            n_batch = [n_batch]\n",
    "        \n",
    "        self.n_batch = n_batch\n",
    "        \n",
    "        self._build_encoder()\n",
    "        self._build_decoder()\n",
    "    \n",
    "    def _build_encoder(self):\n",
    "        layers = []\n",
    "        \n",
    "        input_size = self.n_input_feature\n",
    "        if not self.batch_invariant:\n",
    "            input_size += sum(self.n_batch)\n",
    "        \n",
    "        in_features = input_size\n",
    "        for units in self.encoder_layer:\n",
    "            layers.append(nn.Linear(in_features, units))\n",
    "            layers.append(nn.BatchNorm1d(units))\n",
    "            layers.append(nn.ELU())\n",
    "            in_features = units\n",
    "        \n",
    "        self.encoder_hidden = nn.Sequential(*layers)\n",
    "        \n",
    "        if self.latent_dist == \"normal\":\n",
    "            self.z_mu_layer = nn.Linear(self.encoder_layer[-1], self.z_dim)\n",
    "            self.z_sigma_square_layer = nn.Linear(self.encoder_layer[-1], self.z_dim)\n",
    "        elif self.latent_dist == \"vmf\":\n",
    "            self.z_mu_layer = nn.Linear(self.encoder_layer[-1], self.z_dim)\n",
    "            self.z_sigma_square_layer = nn.Linear(self.encoder_layer[-1], 1)\n",
    "        elif self.latent_dist == \"wn\":\n",
    "            self.z_mu_layer = nn.Linear(self.encoder_layer[-1], self.z_dim)\n",
    "            self.z_sigma_square_layer = nn.Linear(self.encoder_layer[-1], self.z_dim)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "    \n",
    "    def _build_decoder(self):\n",
    "        layers = []\n",
    "        \n",
    "        input_size = self.z_dim + sum(self.n_batch)\n",
    "        \n",
    "        in_features = input_size\n",
    "        for units in self.decoder_layer:\n",
    "            layers.append(nn.Linear(in_features, units))\n",
    "            layers.append(nn.BatchNorm1d(units))\n",
    "            layers.append(nn.ELU())\n",
    "            in_features = units\n",
    "        \n",
    "        self.decoder_hidden = nn.Sequential(*layers)\n",
    "        \n",
    "        self.mu_layer = nn.Linear(self.decoder_layer[-1], self.n_input_feature)\n",
    "        self.sigma_square_layer = nn.Linear(self.decoder_layer[-1], self.n_input_feature)\n",
    "    \n",
    "    def multi_one_hot(self, index_tensor, depth_list):\n",
    "        # Convert multiple batch indices to one-hot encodings\n",
    "        batch_size = index_tensor.size(0)\n",
    "        one_hot_tensor = torch.zeros(batch_size, sum(depth_list), device=index_tensor.device)\n",
    "        \n",
    "        start_idx = 0\n",
    "        for col in range(len(depth_list)):\n",
    "            indices = index_tensor[:, col]\n",
    "            for i in range(batch_size):\n",
    "                if indices[i] < depth_list[col]:\n",
    "                    one_hot_tensor[i, start_idx + indices[i]] = 1\n",
    "            start_idx += depth_list[col]\n",
    "        \n",
    "        return one_hot_tensor\n",
    "    \n",
    "    def _encoder(self, x, batch):\n",
    "        if self.observation_dist == \"nb\":\n",
    "            x = torch.log1p(x)\n",
    "            \n",
    "            if self.latent_dist == \"vmf\":\n",
    "                x = F.normalize(x, p=2, dim=-1)\n",
    "        \n",
    "        if not self.batch_invariant:\n",
    "            x = torch.cat([x, batch], dim=1)\n",
    "        \n",
    "        h = self.encoder_hidden(x)\n",
    "        \n",
    "        if self.latent_dist == \"normal\":\n",
    "            z_mu = self.z_mu_layer(h)\n",
    "            z_sigma_square = F.softplus(self.z_sigma_square_layer(h))\n",
    "        elif self.latent_dist == \"vmf\":\n",
    "            z_mu = self.z_mu_layer(h)\n",
    "            z_mu = F.normalize(z_mu, p=2, dim=-1)\n",
    "            z_sigma_square = F.softplus(self.z_sigma_square_layer(h)) + 1\n",
    "            z_sigma_square = torch.clamp(z_sigma_square, 1, 10000)\n",
    "        elif self.latent_dist == \"wn\":\n",
    "            z_mu = self.z_mu_layer(h)\n",
    "            z_mu = self._polar_project(z_mu)\n",
    "            z_sigma_square = F.softplus(self.z_sigma_square_layer(h))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        return z_mu, z_sigma_square\n",
    "    \n",
    "    def _decoder(self, z, batch):\n",
    "        z = torch.cat([z, batch], dim=1)\n",
    "        \n",
    "        h = self.decoder_hidden(z)\n",
    "        \n",
    "        if self.observation_dist == \"nb\":\n",
    "            mu = F.softmax(self.mu_layer(h), dim=1)\n",
    "            mu = mu * self.library_size\n",
    "            \n",
    "            sigma_square = F.softplus(self.sigma_square_layer(h))\n",
    "            sigma_square = torch.mean(sigma_square, dim=0)\n",
    "        else:\n",
    "            mu = self.mu_layer(h)\n",
    "            sigma_square = F.softplus(self.sigma_square_layer(h))\n",
    "        \n",
    "        sigma_square = torch.clamp(sigma_square, self.EPS, self.MAX_SIGMA_SQUARE)\n",
    "        \n",
    "        return mu, sigma_square\n",
    "    \n",
    "    def _clip_min_value(self, x, eps=1e-6):\n",
    "        return F.relu(x - eps) + eps\n",
    "    \n",
    "    def _polar_project(self, x):\n",
    "        x_norm = torch.sum(torch.square(x), dim=1, keepdim=True)\n",
    "        x_norm = torch.sqrt(self._clip_min_value(x_norm))\n",
    "        \n",
    "        x_unit = x / x_norm\n",
    "        x_norm = torch.clamp(x_norm, 0, 32)\n",
    "        \n",
    "        z = torch.cat([\n",
    "            torch.cosh(x_norm), \n",
    "            torch.sinh(x_norm) * x_unit\n",
    "        ], dim=1)\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def _depth_regularizer(self, x, batch):\n",
    "        with torch.no_grad():\n",
    "            rate = x * 0.2\n",
    "            samples = torch.poisson(rate)\n",
    "        \n",
    "        x_perturbed = F.relu(x - samples)\n",
    "        z_mu_hat, _ = self._encoder(x_perturbed, batch)\n",
    "        \n",
    "        mean_diff = torch.sum(torch.pow(self.z_mu - z_mu_hat, 2), dim=1)\n",
    "        loss = torch.mean(mean_diff)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def log_likelihood_nb(self, x, mu, sigma_square, eps=1e-10):\n",
    "        log_theta_mu_eps = torch.log(sigma_square + mu + eps)\n",
    "        res = (\n",
    "            torch.lgamma(x + sigma_square) - torch.lgamma(x + 1) - torch.lgamma(sigma_square) +\n",
    "            sigma_square * (torch.log(sigma_square) - log_theta_mu_eps) +\n",
    "            x * (torch.log(mu) - log_theta_mu_eps)\n",
    "        )\n",
    "        return res\n",
    "    \n",
    "    def log_likelihood_student(self, x, mu, sigma_square, df=5.0):\n",
    "        df_halved = df / 2\n",
    "        return (\n",
    "            torch.lgamma(df_halved + 0.5) - torch.lgamma(df_halved) -\n",
    "            0.5 * torch.log(math.pi * df * sigma_square) -\n",
    "            (df_halved + 0.5) * torch.log(1 + (x - mu)**2 / (df * sigma_square))\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, batch_id):\n",
    "        if len(self.n_batch) > 1:\n",
    "            batch = self.multi_one_hot(batch_id, self.n_batch)\n",
    "        else:\n",
    "            batch = F.one_hot(batch_id, self.n_batch[0]).float()\n",
    "        \n",
    "        self.library_size = torch.sum(x, dim=1, keepdim=True)\n",
    "        \n",
    "        self.z_mu, self.z_sigma_square = self._encoder(x, batch)\n",
    "        \n",
    "        # Sample latent variable\n",
    "        if self.latent_dist == \"normal\":\n",
    "            self.q_z = dist.Normal(self.z_mu, torch.sqrt(self.z_sigma_square))\n",
    "            self.z = self.q_z.rsample()\n",
    "            self.p_z = dist.Normal(torch.zeros_like(self.z), torch.ones_like(self.z))\n",
    "            kl = dist.kl_divergence(self.q_z, self.p_z).sum(dim=1)\n",
    "            self.kl = torch.mean(kl)\n",
    "        elif self.latent_dist == 'vmf':\n",
    "            self.q_z = VonMisesFisher(self.z_mu, self.z_sigma_square)\n",
    "            self.z = self.q_z.sample()\n",
    "            self.p_z = HypersphericalUniform(self.z_dim - 1, dtype=x.dtype)\n",
    "            kl = self.q_z.kl_divergence(self.p_z)\n",
    "            self.kl = torch.mean(kl)\n",
    "        elif self.latent_dist == 'wn':\n",
    "            self.q_z = HyperbolicWrappedNorm(self.z_mu, self.z_sigma_square)\n",
    "            self.z = self.q_z.sample()\n",
    "            tmp = self._polar_project(torch.zeros_like(self.z_sigma_square))\n",
    "            self.p_z = HyperbolicWrappedNorm(tmp, torch.ones_like(self.z_sigma_square))\n",
    "            kl = self.q_z.log_prob(self.z) - self.p_z.log_prob(self.z)\n",
    "            self.kl = torch.mean(kl)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        # Decoder\n",
    "        self.mu, self.sigma_square = self._decoder(self.z, batch)\n",
    "        \n",
    "        # Depth regularization\n",
    "        self.depth_loss = self._depth_regularizer(x, batch)\n",
    "        \n",
    "        # ELBO calculation\n",
    "        if"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
