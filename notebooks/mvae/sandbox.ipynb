{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.special\n",
    "import torch\n",
    "import torch.distributions as distributions\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-8\n",
    "max_norm = 85\n",
    "ln_2 = math.log(2)\n",
    "ln_pi = math.log(math.pi)\n",
    "ln_2pi = ln_2 + ln_pi\n",
    "\n",
    "\n",
    "class LeakyClamp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, min, max):\n",
    "        ctx.save_for_backward(x.ge(min) * x.le(max))\n",
    "        return torch.clamp(x, min=min, max=max)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        mask, = ctx.saved_tensors\n",
    "        mask = mask.type_as(grad_output)\n",
    "        return grad_output * mask + grad_output * (1 - mask) * eps, None, None\n",
    "\n",
    "\n",
    "def clamp(x, min=float(\"-inf\"), max=float(\"+inf\")):\n",
    "    return LeakyClamp.apply(x, min, max)\n",
    "\n",
    "\n",
    "def cosh(x):\n",
    "    x = clamp(x, min=-max_norm, max=max_norm)\n",
    "    return torch.cosh(x)\n",
    "\n",
    "\n",
    "def sinh(x):\n",
    "    x = clamp(x, min=-max_norm, max=max_norm)\n",
    "    return torch.sinh(x)\n",
    "\n",
    "\n",
    "class Atanh(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        x = clamp(x, min=-1. + 4 * eps, max=1. - 4 * eps)\n",
    "        ctx.save_for_backward(x)\n",
    "        res = (torch.log_(1 + x).sub_(torch.log_(1 - x))).mul_(0.5)\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, = ctx.saved_tensors\n",
    "        return grad_output / (1 - x**2)\n",
    "\n",
    "\n",
    "def atanh(x):\n",
    "    return Atanh.apply(x)\n",
    "\n",
    "\n",
    "class Acosh(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        x = clamp(x, min=1 + eps)\n",
    "        z = sqrt(x * x - 1.)\n",
    "        ctx.save_for_backward(z)\n",
    "        return torch.log(x + z)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        z, = ctx.saved_tensors\n",
    "        z_ = z\n",
    "        return grad_output / z_\n",
    "\n",
    "\n",
    "def acosh(x):\n",
    "    return Acosh.apply(x)\n",
    "\n",
    "\n",
    "def sqrt(x):\n",
    "    x = clamp(x, min=1e-9)\n",
    "    return torch.sqrt(x)\n",
    "\n",
    "\n",
    "def logcosh(x):\n",
    "    x_exp = x.unsqueeze(dim=-1)\n",
    "    value = torch.cat((torch.zeros_like(x_exp), -2. * x_exp), dim=-1)\n",
    "    return x + torch.logsumexp(value, dim=-1) - ln_2\n",
    "\n",
    "\n",
    "def logsinh(x):\n",
    "    x_exp = x.unsqueeze(dim=-1)\n",
    "    signs = torch.cat((torch.ones_like(x_exp), -torch.ones_like(x_exp)), dim=-1)\n",
    "    value = torch.cat((torch.zeros_like(x_exp), -2. * x_exp), dim=-1)\n",
    "    return x + logsumexp_signs(value, dim=-1, signs=signs) - ln_2\n",
    "\n",
    "\n",
    "def logsumexp_signs(value, dim=0, keepdim=False, signs=None):\n",
    "    if signs is None:\n",
    "        signs = torch.ones_like(value)\n",
    "    m, _ = torch.max(value, dim=dim, keepdim=True)\n",
    "    value0 = value - m\n",
    "    if keepdim is False:\n",
    "        m = m.squeeze(dim)\n",
    "    return m + torch.log(clamp(torch.sum(signs * torch.exp(value0), dim=dim, keepdim=keepdim), min=eps))\n",
    "\n",
    "\n",
    "def expand_proj_dims(x):\n",
    "    zeros = torch.zeros(x.shape[:-1] + torch.Size([1])).to(x.device).to(x.dtype)\n",
    "    return torch.cat((zeros, x), dim=-1)\n",
    "\n",
    "\n",
    "class IveFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, v, z):\n",
    "        ctx.save_for_backward(z)\n",
    "        ctx.v = v\n",
    "        z_cpu = z.double().detach().cpu().numpy()\n",
    "        if np.isclose(v, 0):\n",
    "            output = scipy.special.i0e(z_cpu, dtype=z_cpu.dtype)\n",
    "        elif np.isclose(v, 1):\n",
    "            output = scipy.special.i1e(z_cpu, dtype=z_cpu.dtype)\n",
    "        else:\n",
    "            output = scipy.special.ive(v, z_cpu, dtype=z_cpu.dtype)\n",
    "        return torch.tensor(output, dtype=z.dtype, device=z.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        z = ctx.saved_tensors[-1]\n",
    "        return None, grad_output * (ive(ctx.v - 1, z) - ive(ctx.v, z) * (ctx.v + z) / z)\n",
    "\n",
    "\n",
    "def ive(v, z):\n",
    "    return IveFunction.apply(v, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributions\n",
    "class EuclideanNormal(distributions.Normal):\n",
    "    def log_prob(self, value):\n",
    "        return super().log_prob(value).sum(dim=-1)\n",
    "\n",
    "\n",
    "class EuclideanUniform(distributions.Uniform):\n",
    "    def log_prob(self, value):\n",
    "        return super().log_prob(value).sum(dim=-1)\n",
    "\n",
    "\n",
    "class HypersphericalUniform(distributions.Distribution):\n",
    "    arg_constraints = {}\n",
    "    support = distributions.constraints.real\n",
    "    _mean_carrier_measure = 0\n",
    "\n",
    "    def __init__(self, dim, validate_args=None, device=\"cuda\"):\n",
    "        self.dim = dim\n",
    "        self.device = device\n",
    "        self.normal = EuclideanNormal(0, 1)\n",
    "        super().__init__(torch.Size([dim]), validate_args=validate_args)\n",
    "\n",
    "    def rsample(self, sample_shape):\n",
    "        output = self.normal.sample(sample_shape + torch.Size([self.dim + 1])).to(self.device)\n",
    "        return F.normalize(output, dim=-1)\n",
    "\n",
    "    def entropy(self):\n",
    "        return self.__log_surface_area()\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        return -torch.ones(x.shape[:-1], device=self.device) * self.__log_surface_area()\n",
    "\n",
    "    def __log_surface_area(self):\n",
    "        n = self.dim\n",
    "        t = torch.tensor((n + 1.) / 2.)\n",
    "        ret = ln_2 + t * ln_pi - torch.lgamma(t)\n",
    "        return ret\n",
    "\n",
    "\n",
    "class VonMisesFisher(distributions.Distribution):\n",
    "    arg_constraints = {\n",
    "        \"loc\": distributions.constraints.real,\n",
    "        \"scale\": distributions.constraints.positive\n",
    "    }\n",
    "    support = distributions.constraints.real\n",
    "    _mean_carrier_measure = 0\n",
    "\n",
    "    def __init__(self, loc, scale, validate_args=None, device=\"cuda\"):\n",
    "\n",
    "        assert (scale > 0).all()\n",
    "        assert loc.norm(p=2, dim=-1).allclose(torch.ones(loc.shape[:-1], device=loc.device))\n",
    "\n",
    "        self.dtype = loc.dtype\n",
    "        self.loc = loc.to(device)\n",
    "        self.scale = scale.to(device)\n",
    "        self.device = device\n",
    "        self.p = loc.shape[-1]\n",
    "        self.uniform = EuclideanUniform(0, 1)\n",
    "        self.hyperspherical_uniform_v = HypersphericalUniform(self.p - 2, device=self.device)\n",
    "        e1 = torch.tensor([1.] + [0.] * (loc.shape[-1] - 1), requires_grad=False, device=self.device)\n",
    "        self.u = F.normalize(e1 - self.loc)\n",
    "        super().__init__(self.loc.size(), validate_args=validate_args)\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        return self.loc * (ive(self.p / 2, self.scale) / ive(self.p / 2 - 1, self.scale))\n",
    "\n",
    "    @property\n",
    "    def stddev(self):\n",
    "        return self.scale\n",
    "\n",
    "    def rsample(self, sample_shape):\n",
    "        shape = sample_shape if isinstance(sample_shape, torch.Size) else torch.Size([sample_shape])\n",
    "        w = self._sample_w3(shape=shape) if self.p == 3 else self._sample_w_rej(shape=shape)\n",
    "        v = self.hyperspherical_uniform_v.sample(shape)\n",
    "        w_ = sqrt(1 - w ** 2)\n",
    "        print(w.shape, w_.shape, v.shape)\n",
    "        x = torch.cat((w, w_ * v), dim=-1)\n",
    "        z = self._householder_rotation(x)\n",
    "        return z.to(dtype=self.dtype)\n",
    "    \n",
    "    def _sample_w3(self, shape):\n",
    "        shape = torch.Size(shape + torch.Size(self.scale.shape))\n",
    "        u = self.uniform.sample(shape).to(self.device)\n",
    "        log_u = torch.log(u)\n",
    "        inv_log_u = torch.log(1 - u) - 2 * self.scale\n",
    "        stack = torch.stack([log_u, inv_log_u], dim=0)\n",
    "        w = 1 + stack.logsumexp(dim=0) / self.scale\n",
    "        self.__w = torch.clamp(w, min=-1, max=1)\n",
    "        return self.__w\n",
    "\n",
    "    def _sample_w_rej(self, shape):\n",
    "        c = torch.sqrt((4 * (self.scale ** 2)) + (self.p - 1) ** 2)\n",
    "        b_true = (-2 * self.scale + c) / (self.p - 1)\n",
    "        b_app = (self.p - 1) / (4 * self.scale)\n",
    "        s = torch.min(\n",
    "            torch.max(torch.tensor([0.], device=self.device), self.scale - 10),\n",
    "            torch.tensor([1.], device=self.device)\n",
    "        )\n",
    "        b = b_app * s + b_true * (1 - s)\n",
    "        a = (self.p - 1 + 2 * self.scale + c) / 4\n",
    "        d = (4 * a * b) / (1 + b) - (self.p - 1) * math.log(self.p - 1)\n",
    "        self.__b, (self.__e, self.__w) = b, self._while_loop(b, a, d, shape)\n",
    "        return self.__w\n",
    "\n",
    "    def _while_loop(self, b, a, d, shape):\n",
    "        b, a, d = [e.repeat(*shape, *([1] * len(self.scale.shape))) for e in (b, a, d)]\n",
    "        w = torch.zeros_like(b).to(self.device)\n",
    "        e = torch.zeros_like(b).to(self.device)\n",
    "        bool_mask = (torch.ones_like(b) == 1).to(self.device)\n",
    "        shape = torch.Size(shape + torch.Size(self.scale.shape))\n",
    "        while bool_mask.sum() != 0:\n",
    "            e_ = torch.distributions.Beta(\n",
    "                (self.p - 1) / 2,\n",
    "                (self.p - 1) / 2\n",
    "            ).sample(shape[:-1]).reshape(shape).to(self.device)\n",
    "            u = self.uniform.sample(shape).to(self.device)\n",
    "            w_ = (1 - (1 + b) * e_) / (1 - (1 - b) * e_)\n",
    "            t = (2 * a * b) / (1 - (1 - b) * e_)\n",
    "            accept = ((self.p - 1) * t.log() - t + d) > torch.log(u)\n",
    "            reject = ~accept\n",
    "            w[bool_mask * accept] = w_[bool_mask * accept]\n",
    "            e[bool_mask * accept] = e_[bool_mask * accept]\n",
    "            bool_mask[bool_mask * accept] = reject[bool_mask * accept]\n",
    "        return e, w\n",
    "\n",
    "    def _householder_rotation(self, x):\n",
    "        z = x - 2 * (x * self.u).sum(-1, keepdim=True) * self.u\n",
    "        return z\n",
    "\n",
    "    def entropy(self):\n",
    "        ive_ = ive((self.p / 2) - 1, self.scale)\n",
    "        output = -self.scale * ive(self.p / 2, self.scale) / ive_\n",
    "        return output.view(*(output.shape[:-1])) - self._c_p_kappa(self.scale, p=self.p, ive_precomp=ive_)\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        assert torch.norm(x, p=2, dim=-1).allclose(torch.ones(x.shape[:-1], dtype=x.dtype, device=x.device))\n",
    "        expprob = self._log_unnormalized_prob(x)\n",
    "        norm_const = self._c_p_kappa(self.scale, p=self.p)\n",
    "        output = expprob + norm_const\n",
    "        return output\n",
    "\n",
    "    def _log_unnormalized_prob(self, x): \n",
    "        output = self.scale * (self.loc * x).sum(dim=-1, keepdim=True)\n",
    "        return output.view(*(output.shape[:-1]))\n",
    "\n",
    "    @staticmethod\n",
    "    def _c_p_kappa(kappa, p, ive_precomp=None):\n",
    "        ln_kappa = torch.log(kappa)\n",
    "        if ive_precomp is not None:\n",
    "            ive_ = ive_precomp\n",
    "        else:\n",
    "            ive_ = ive(p / 2 - 1, kappa)\n",
    "        ln_ive_ = torch.log(ive_)\n",
    "        ln_iv_ = ln_ive_ + kappa\n",
    "        output = p * (ln_kappa - math.log(2 * math.pi)) / 2. - ln_kappa - ln_iv_ \n",
    "        return output.view(*(output.shape[:-1]))\n",
    "    \n",
    "\n",
    "class HyperbolicWrappedNormal(distributions.Distribution):\n",
    "    arg_constraints = {\n",
    "        \"loc\": distributions.constraints.real_vector,\n",
    "        \"scale\": distributions.constraints.positive\n",
    "    }\n",
    "    support = distributions.constraints.real\n",
    "\n",
    "    def __init__(self, loc, scale, radius=1.0, validate_args=None, device=\"cuda\"):\n",
    "        self.dim = loc.shape[-1]\n",
    "        tangent_dim = self.dim - 1\n",
    "\n",
    "        if scale.shape[-1] > 1 and scale.shape[-1] != tangent_dim:\n",
    "            raise ValueError()\n",
    "\n",
    "        if scale.shape[-1] == 1:\n",
    "            s = [1] * len(scale.shape)\n",
    "            s[-1] = tangent_dim\n",
    "            scale = scale.repeat(s)\n",
    "\n",
    "        assert loc.shape[:-1] == scale.shape[:-1]\n",
    "        assert tangent_dim == scale.shape[-1]\n",
    "\n",
    "        self.loc = loc.to(device)\n",
    "        self.scale = scale.to(device)\n",
    "        self.radius = radius\n",
    "        self.device = device\n",
    "        smaller_shape = self.loc.shape[:-1] + torch.Size([tangent_dim])\n",
    "        self.normal = EuclideanNormal(torch.zeros(smaller_shape, device=self.device), self.scale)\n",
    "        super().__init__(self.loc.size(), validate_args=validate_args)\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        return self.loc\n",
    "\n",
    "    @property\n",
    "    def stddev(self):\n",
    "        return self.scale\n",
    "\n",
    "    def _lorentz_product(self, x, y, keepdim=False, dim=-1):\n",
    "        m = x * y\n",
    "        if keepdim:\n",
    "            ret = torch.sum(m, dim=dim, keepdim=True) - 2 * m[..., 0:1]\n",
    "        else:\n",
    "            ret = torch.sum(m, dim=dim, keepdim=False) - 2 * m[..., 0]\n",
    "        return ret\n",
    "\n",
    "    def _lorentz_norm(self, x, keepdim=True):\n",
    "        product = self._lorentz_product(x, x, keepdim=keepdim)\n",
    "        ret = sqrt(product)\n",
    "        return ret\n",
    "\n",
    "    def _parallel_transport(self, x, dst, radius):\n",
    "        denom = radius * (radius + dst[..., 0:1])\n",
    "        lp = self._lorentz_product(dst, x, keepdim=True)\n",
    "        coef = lp / denom\n",
    "        right = torch.cat((dst[..., :1] + radius, dst[..., 1:]), dim=-1)\n",
    "        return x + coef * right\n",
    "    \n",
    "    def _inverse_parallel_transport(self, x, src, radius):\n",
    "        denom = (radius + src[..., 0:1])\n",
    "        lp = -x[..., 0:1]\n",
    "        coef = lp / denom\n",
    "        right = torch.cat((src[..., 0:1] + radius, src[..., 1:]), dim=-1)\n",
    "        return x + coef * right\n",
    "\n",
    "    def _exponential_map(self, x, at_point, radius):\n",
    "        x_norm = self._lorentz_norm(x, keepdim=True) / radius\n",
    "        x_normed = x / x_norm\n",
    "        ret = cosh(x_norm) * at_point + sinh(x_norm) * x_normed\n",
    "        return ret\n",
    "\n",
    "    def _inverse_exponential_map(self, x, at_point, radius):\n",
    "        alpha = -self._lorentz_product(at_point, x, keepdim=True) / (radius ** 2)\n",
    "        coef = acosh(alpha) / sqrt(alpha ** 2 - 1)\n",
    "        ret = coef * (x - alpha * at_point)\n",
    "        return ret\n",
    "\n",
    "    def _sample_projection(self, x, at_point, radius):\n",
    "        x = expand_proj_dims(x)\n",
    "        pt = self._parallel_transport(x, dst=at_point, radius=radius)\n",
    "        x_proj = self._exponential_map(pt, at_point=at_point, radius=radius)\n",
    "        return x_proj, (pt, x)\n",
    "    \n",
    "    def _inverse_sample_projection(self, x, at_point, radius):\n",
    "        unmapped = self._inverse_exponential_map(x, at_point=at_point, radius=radius)\n",
    "        unpt = self._inverse_parallel_transport(unmapped, src=at_point, radius=radius)\n",
    "        return unmapped, unpt[..., 1:]\n",
    "    \n",
    "    def _logdet(self, u, radius):\n",
    "        r = self._lorentz_norm(u, keepdim=True) / radius\n",
    "        n = u.shape[-1] - 1\n",
    "        logdet_partial = (n - 1) * (torch.log(radius) + logsinh(r) - torch.log(r))\n",
    "        return logdet_partial\n",
    "\n",
    "    def rsample_with_parts(self, sample_shape=torch.Size()):\n",
    "        v_tilde = self.normal.rsample(sample_shape).to(self.device)\n",
    "        v = expand_proj_dims(v_tilde)\n",
    "        pt = self._parallel_transport(v, dst=self.loc, radius=self.radius)\n",
    "        z = self._exponential_map(pt, at_point=self.loc, radius=self.radius)\n",
    "        return z, (pt, v_tilde)\n",
    "\n",
    "    def rsample(self, sample_shape=torch.Size()):\n",
    "        z, _ = self.rsample_with_parts(sample_shape)\n",
    "        return z\n",
    "\n",
    "    def log_prob_from_parts(self, z, data):\n",
    "        v = data[1]\n",
    "        n_logprob = self.normal.log_prob(v)\n",
    "        logdet = self._logdet(data[0], self.radius)\n",
    "        log_prob = n_logprob - logdet\n",
    "        return log_prob\n",
    "\n",
    "    def log_prob(self, z):\n",
    "        data = self._inverse_sample_projection(z, at_point=self.loc)\n",
    "        return self.log_prob_from_parts(z, data)\n",
    "\n",
    "    def rsample_log_prob(self, sample_shape=torch.Size()):\n",
    "        z, data = self.rsample_with_parts(sample_shape)\n",
    "        return z, self.log_prob_from_parts(z, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = HyperbolicWrappedNormal(loc=torch.tensor([1.0, 0.0, 0.0]), scale=torch.tensor([1.0]), device=\"cuda\")\n",
    "samples = wn.rsample(torch.Size([1000]))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "ax.scatter(samples[:, 0].cpu().numpy(), samples[:, 1].cpu().numpy())\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(-10, 10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "import math\n",
    "\n",
    "class MixedCurvatureVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_gene, \n",
    "        n_batch=None, \n",
    "        z_dim=2,\n",
    "        encoder_layer=None, \n",
    "        decoder_layer=None, \n",
    "        latent_dist=\"vmf\", \n",
    "        observation_dist=\"nb\",\n",
    "        batch_invariant=False\n",
    "    ):\n",
    "        super(MixedCurvatureVAE, self).__init__()\n",
    "        \n",
    "        if encoder_layer is None:\n",
    "            encoder_layer = [128, 64, 32]\n",
    "        if decoder_layer is None:\n",
    "            decoder_layer = [32, 128]\n",
    "        \n",
    "        self.batch_invariant = batch_invariant\n",
    "        self.n_input_feature = n_gene\n",
    "        self.z_dim = z_dim\n",
    "        self.encoder_layer = encoder_layer\n",
    "        self.decoder_layer = decoder_layer\n",
    "        self.latent_dist = latent_dist\n",
    "        self.observation_dist = observation_dist\n",
    "        \n",
    "        if self.latent_dist == \"vmf\":\n",
    "            self.z_dim += 1\n",
    "        \n",
    "        if not isinstance(n_batch, list):\n",
    "            n_batch = [n_batch]\n",
    "        \n",
    "        self.n_batch = n_batch\n",
    "        \n",
    "        self._build_encoder()\n",
    "        self._build_decoder()\n",
    "    \n",
    "    def _build_encoder(self):\n",
    "        layers = []\n",
    "        \n",
    "        input_size = self.n_input_feature\n",
    "        if not self.batch_invariant:\n",
    "            input_size += sum(self.n_batch)\n",
    "        \n",
    "        in_features = input_size\n",
    "        for units in self.encoder_layer:\n",
    "            layers.append(nn.Linear(in_features, units))\n",
    "            layers.append(nn.BatchNorm1d(units))\n",
    "            layers.append(nn.ELU())\n",
    "            in_features = units\n",
    "        \n",
    "        self.encoder_hidden = nn.Sequential(*layers)\n",
    "        \n",
    "        if self.latent_dist == \"normal\":\n",
    "            self.z_mu_layer = nn.Linear(self.encoder_layer[-1], self.z_dim)\n",
    "            self.z_sigma_square_layer = nn.Linear(self.encoder_layer[-1], self.z_dim)\n",
    "        elif self.latent_dist == \"vmf\":\n",
    "            self.z_mu_layer = nn.Linear(self.encoder_layer[-1], self.z_dim)\n",
    "            self.z_sigma_square_layer = nn.Linear(self.encoder_layer[-1], 1)\n",
    "        elif self.latent_dist == \"wn\":\n",
    "            self.z_mu_layer = nn.Linear(self.encoder_layer[-1], self.z_dim)\n",
    "            self.z_sigma_square_layer = nn.Linear(self.encoder_layer[-1], self.z_dim)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "    \n",
    "    def _build_decoder(self):\n",
    "        layers = []\n",
    "        \n",
    "        input_size = self.z_dim + sum(self.n_batch)\n",
    "        \n",
    "        in_features = input_size\n",
    "        for units in self.decoder_layer:\n",
    "            layers.append(nn.Linear(in_features, units))\n",
    "            layers.append(nn.BatchNorm1d(units))\n",
    "            layers.append(nn.ELU())\n",
    "            in_features = units\n",
    "        \n",
    "        self.decoder_hidden = nn.Sequential(*layers)\n",
    "        \n",
    "        self.mu_layer = nn.Linear(self.decoder_layer[-1], self.n_input_feature)\n",
    "        self.sigma_square_layer = nn.Linear(self.decoder_layer[-1], self.n_input_feature)\n",
    "    \n",
    "    def multi_one_hot(self, index_tensor, depth_list):\n",
    "        # Convert multiple batch indices to one-hot encodings\n",
    "        batch_size = index_tensor.size(0)\n",
    "        one_hot_tensor = torch.zeros(batch_size, sum(depth_list), device=index_tensor.device)\n",
    "        \n",
    "        start_idx = 0\n",
    "        for col in range(len(depth_list)):\n",
    "            indices = index_tensor[:, col]\n",
    "            for i in range(batch_size):\n",
    "                if indices[i] < depth_list[col]:\n",
    "                    one_hot_tensor[i, start_idx + indices[i]] = 1\n",
    "            start_idx += depth_list[col]\n",
    "        \n",
    "        return one_hot_tensor\n",
    "    \n",
    "    def _encoder(self, x, batch):\n",
    "        if self.observation_dist == \"nb\":\n",
    "            x = torch.log1p(x)\n",
    "            \n",
    "            if self.latent_dist == \"vmf\":\n",
    "                x = F.normalize(x, p=2, dim=-1)\n",
    "        \n",
    "        if not self.batch_invariant:\n",
    "            x = torch.cat([x, batch], dim=1)\n",
    "        \n",
    "        h = self.encoder_hidden(x)\n",
    "        \n",
    "        if self.latent_dist == \"normal\":\n",
    "            z_mu = self.z_mu_layer(h)\n",
    "            z_sigma_square = F.softplus(self.z_sigma_square_layer(h))\n",
    "        elif self.latent_dist == \"vmf\":\n",
    "            z_mu = self.z_mu_layer(h)\n",
    "            z_mu = F.normalize(z_mu, p=2, dim=-1)\n",
    "            z_sigma_square = F.softplus(self.z_sigma_square_layer(h)) + 1\n",
    "            z_sigma_square = torch.clamp(z_sigma_square, 1, 10000)\n",
    "        elif self.latent_dist == \"wn\":\n",
    "            z_mu = self.z_mu_layer(h)\n",
    "            z_mu = self._polar_project(z_mu)\n",
    "            z_sigma_square = F.softplus(self.z_sigma_square_layer(h))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        return z_mu, z_sigma_square\n",
    "    \n",
    "    def _decoder(self, z, batch):\n",
    "        z = torch.cat([z, batch], dim=1)\n",
    "        \n",
    "        h = self.decoder_hidden(z)\n",
    "        \n",
    "        if self.observation_dist == \"nb\":\n",
    "            mu = F.softmax(self.mu_layer(h), dim=1)\n",
    "            mu = mu * self.library_size\n",
    "            \n",
    "            sigma_square = F.softplus(self.sigma_square_layer(h))\n",
    "            sigma_square = torch.mean(sigma_square, dim=0)\n",
    "        else:\n",
    "            mu = self.mu_layer(h)\n",
    "            sigma_square = F.softplus(self.sigma_square_layer(h))\n",
    "        \n",
    "        sigma_square = torch.clamp(sigma_square, self.EPS, self.MAX_SIGMA_SQUARE)\n",
    "        \n",
    "        return mu, sigma_square\n",
    "    \n",
    "    def _clip_min_value(self, x, eps=1e-6):\n",
    "        return F.relu(x - eps) + eps\n",
    "    \n",
    "    def _polar_project(self, x):\n",
    "        x_norm = torch.sum(torch.square(x), dim=1, keepdim=True)\n",
    "        x_norm = torch.sqrt(self._clip_min_value(x_norm))\n",
    "        \n",
    "        x_unit = x / x_norm\n",
    "        x_norm = torch.clamp(x_norm, 0, 32)\n",
    "        \n",
    "        z = torch.cat([\n",
    "            torch.cosh(x_norm), \n",
    "            torch.sinh(x_norm) * x_unit\n",
    "        ], dim=1)\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def _depth_regularizer(self, x, batch):\n",
    "        with torch.no_grad():\n",
    "            rate = x * 0.2\n",
    "            samples = torch.poisson(rate)\n",
    "        \n",
    "        x_perturbed = F.relu(x - samples)\n",
    "        z_mu_hat, _ = self._encoder(x_perturbed, batch)\n",
    "        \n",
    "        mean_diff = torch.sum(torch.pow(self.z_mu - z_mu_hat, 2), dim=1)\n",
    "        loss = torch.mean(mean_diff)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def log_likelihood_nb(self, x, mu, sigma_square, eps=1e-10):\n",
    "        log_theta_mu_eps = torch.log(sigma_square + mu + eps)\n",
    "        res = (\n",
    "            torch.lgamma(x + sigma_square) - torch.lgamma(x + 1) - torch.lgamma(sigma_square) +\n",
    "            sigma_square * (torch.log(sigma_square) - log_theta_mu_eps) +\n",
    "            x * (torch.log(mu) - log_theta_mu_eps)\n",
    "        )\n",
    "        return res\n",
    "    \n",
    "    def log_likelihood_student(self, x, mu, sigma_square, df=5.0):\n",
    "        df_halved = df / 2\n",
    "        return (\n",
    "            torch.lgamma(df_halved + 0.5) - torch.lgamma(df_halved) -\n",
    "            0.5 * torch.log(math.pi * df * sigma_square) -\n",
    "            (df_halved + 0.5) * torch.log(1 + (x - mu)**2 / (df * sigma_square))\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, batch_id):\n",
    "        if len(self.n_batch) > 1:\n",
    "            batch = self.multi_one_hot(batch_id, self.n_batch)\n",
    "        else:\n",
    "            batch = F.one_hot(batch_id, self.n_batch[0]).float()\n",
    "        \n",
    "        self.library_size = torch.sum(x, dim=1, keepdim=True)\n",
    "        \n",
    "        self.z_mu, self.z_sigma_square = self._encoder(x, batch)\n",
    "        \n",
    "        # Sample latent variable\n",
    "        if self.latent_dist == \"normal\":\n",
    "            self.q_z = dist.Normal(self.z_mu, torch.sqrt(self.z_sigma_square))\n",
    "            self.z = self.q_z.rsample()\n",
    "            self.p_z = dist.Normal(torch.zeros_like(self.z), torch.ones_like(self.z))\n",
    "            kl = dist.kl_divergence(self.q_z, self.p_z).sum(dim=1)\n",
    "            self.kl = torch.mean(kl)\n",
    "        elif self.latent_dist == 'vmf':\n",
    "            self.q_z = VonMisesFisher(self.z_mu, self.z_sigma_square)\n",
    "            self.z = self.q_z.sample()\n",
    "            self.p_z = HypersphericalUniform(self.z_dim - 1, dtype=x.dtype)\n",
    "            kl = self.q_z.kl_divergence(self.p_z)\n",
    "            self.kl = torch.mean(kl)\n",
    "        elif self.latent_dist == 'wn':\n",
    "            self.q_z = HyperbolicWrappedNorm(self.z_mu, self.z_sigma_square)\n",
    "            self.z = self.q_z.sample()\n",
    "            tmp = self._polar_project(torch.zeros_like(self.z_sigma_square))\n",
    "            self.p_z = HyperbolicWrappedNorm(tmp, torch.ones_like(self.z_sigma_square))\n",
    "            kl = self.q_z.log_prob(self.z) - self.p_z.log_prob(self.z)\n",
    "            self.kl = torch.mean(kl)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        # Decoder\n",
    "        self.mu, self.sigma_square = self._decoder(self.z, batch)\n",
    "        \n",
    "        # Depth regularization\n",
    "        self.depth_loss = self._depth_regularizer(x, batch)\n",
    "        \n",
    "        # ELBO calculation\n",
    "        if"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
