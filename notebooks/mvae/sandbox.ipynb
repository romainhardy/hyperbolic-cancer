{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/home/roh3635/hyperbolic-cancer/mvae/mt/mvae\")\n",
    "\n",
    "import torch\n",
    "import mvae.mt.mvae.utils as utils\n",
    "import mvae.mt.mvae.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as dist\n",
    "import math\n",
    "\n",
    "class MixedCurvatureVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_gene, \n",
    "        n_batch=None, \n",
    "        z_dim=2,\n",
    "        encoder_layer=None, \n",
    "        decoder_layer=None, \n",
    "        latent_dist=\"vmf\", \n",
    "        observation_dist=\"nb\",\n",
    "        batch_invariant=False\n",
    "    ):\n",
    "        super(MixedCurvatureVAE, self).__init__()\n",
    "        \n",
    "        if encoder_layer is None:\n",
    "            encoder_layer = [128, 64, 32]\n",
    "        if decoder_layer is None:\n",
    "            decoder_layer = [32, 128]\n",
    "        \n",
    "        self.batch_invariant = batch_invariant\n",
    "        self.n_input_feature = n_gene\n",
    "        self.z_dim = z_dim\n",
    "        self.encoder_layer = encoder_layer\n",
    "        self.decoder_layer = decoder_layer\n",
    "        self.latent_dist = latent_dist\n",
    "        self.observation_dist = observation_dist\n",
    "        \n",
    "        if self.latent_dist == \"vmf\":\n",
    "            self.z_dim += 1\n",
    "        \n",
    "        if not isinstance(n_batch, list):\n",
    "            n_batch = [n_batch]\n",
    "        \n",
    "        self.n_batch = n_batch\n",
    "        \n",
    "        self._build_encoder()\n",
    "        self._build_decoder()\n",
    "    \n",
    "    def _build_encoder(self):\n",
    "        layers = []\n",
    "        \n",
    "        input_size = self.n_input_feature\n",
    "        if not self.batch_invariant:\n",
    "            input_size += sum(self.n_batch)\n",
    "        \n",
    "        in_features = input_size\n",
    "        for units in self.encoder_layer:\n",
    "            layers.append(nn.Linear(in_features, units))\n",
    "            layers.append(nn.BatchNorm1d(units))\n",
    "            layers.append(nn.ELU())\n",
    "            in_features = units\n",
    "        \n",
    "        self.encoder_hidden = nn.Sequential(*layers)\n",
    "        \n",
    "        if self.latent_dist == \"normal\":\n",
    "            self.z_mu_layer = nn.Linear(self.encoder_layer[-1], self.z_dim)\n",
    "            self.z_sigma_square_layer = nn.Linear(self.encoder_layer[-1], self.z_dim)\n",
    "        elif self.latent_dist == \"vmf\":\n",
    "            self.z_mu_layer = nn.Linear(self.encoder_layer[-1], self.z_dim)\n",
    "            self.z_sigma_square_layer = nn.Linear(self.encoder_layer[-1], 1)\n",
    "        elif self.latent_dist == \"wn\":\n",
    "            self.z_mu_layer = nn.Linear(self.encoder_layer[-1], self.z_dim)\n",
    "            self.z_sigma_square_layer = nn.Linear(self.encoder_layer[-1], self.z_dim)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "    \n",
    "    def _build_decoder(self):\n",
    "        layers = []\n",
    "        \n",
    "        input_size = self.z_dim + sum(self.n_batch)\n",
    "        \n",
    "        in_features = input_size\n",
    "        for units in self.decoder_layer:\n",
    "            layers.append(nn.Linear(in_features, units))\n",
    "            layers.append(nn.BatchNorm1d(units))\n",
    "            layers.append(nn.ELU())\n",
    "            in_features = units\n",
    "        \n",
    "        self.decoder_hidden = nn.Sequential(*layers)\n",
    "        \n",
    "        self.mu_layer = nn.Linear(self.decoder_layer[-1], self.n_input_feature)\n",
    "        self.sigma_square_layer = nn.Linear(self.decoder_layer[-1], self.n_input_feature)\n",
    "    \n",
    "    def multi_one_hot(self, index_tensor, depth_list):\n",
    "        # Convert multiple batch indices to one-hot encodings\n",
    "        batch_size = index_tensor.size(0)\n",
    "        one_hot_tensor = torch.zeros(batch_size, sum(depth_list), device=index_tensor.device)\n",
    "        \n",
    "        start_idx = 0\n",
    "        for col in range(len(depth_list)):\n",
    "            indices = index_tensor[:, col]\n",
    "            for i in range(batch_size):\n",
    "                if indices[i] < depth_list[col]:\n",
    "                    one_hot_tensor[i, start_idx + indices[i]] = 1\n",
    "            start_idx += depth_list[col]\n",
    "        \n",
    "        return one_hot_tensor\n",
    "    \n",
    "    def _encoder(self, x, batch):\n",
    "        if self.observation_dist == \"nb\":\n",
    "            x = torch.log1p(x)\n",
    "            \n",
    "            if self.latent_dist == \"vmf\":\n",
    "                x = F.normalize(x, p=2, dim=-1)\n",
    "        \n",
    "        if not self.batch_invariant:\n",
    "            x = torch.cat([x, batch], dim=1)\n",
    "        \n",
    "        h = self.encoder_hidden(x)\n",
    "        \n",
    "        if self.latent_dist == \"normal\":\n",
    "            z_mu = self.z_mu_layer(h)\n",
    "            z_sigma_square = F.softplus(self.z_sigma_square_layer(h))\n",
    "        elif self.latent_dist == \"vmf\":\n",
    "            z_mu = self.z_mu_layer(h)\n",
    "            z_mu = F.normalize(z_mu, p=2, dim=-1)\n",
    "            z_sigma_square = F.softplus(self.z_sigma_square_layer(h)) + 1\n",
    "            z_sigma_square = torch.clamp(z_sigma_square, 1, 10000)\n",
    "        elif self.latent_dist == \"wn\":\n",
    "            z_mu = self.z_mu_layer(h)\n",
    "            z_mu = self._polar_project(z_mu)\n",
    "            z_sigma_square = F.softplus(self.z_sigma_square_layer(h))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        return z_mu, z_sigma_square\n",
    "    \n",
    "    def _decoder(self, z, batch):\n",
    "        z = torch.cat([z, batch], dim=1)\n",
    "        \n",
    "        h = self.decoder_hidden(z)\n",
    "        \n",
    "        if self.observation_dist == \"nb\":\n",
    "            mu = F.softmax(self.mu_layer(h), dim=1)\n",
    "            mu = mu * self.library_size\n",
    "            \n",
    "            sigma_square = F.softplus(self.sigma_square_layer(h))\n",
    "            sigma_square = torch.mean(sigma_square, dim=0)\n",
    "        else:\n",
    "            mu = self.mu_layer(h)\n",
    "            sigma_square = F.softplus(self.sigma_square_layer(h))\n",
    "        \n",
    "        sigma_square = torch.clamp(sigma_square, self.EPS, self.MAX_SIGMA_SQUARE)\n",
    "        \n",
    "        return mu, sigma_square\n",
    "    \n",
    "    def _clip_min_value(self, x, eps=1e-6):\n",
    "        return F.relu(x - eps) + eps\n",
    "    \n",
    "    def _polar_project(self, x):\n",
    "        x_norm = torch.sum(torch.square(x), dim=1, keepdim=True)\n",
    "        x_norm = torch.sqrt(self._clip_min_value(x_norm))\n",
    "        \n",
    "        x_unit = x / x_norm\n",
    "        x_norm = torch.clamp(x_norm, 0, 32)\n",
    "        \n",
    "        z = torch.cat([\n",
    "            torch.cosh(x_norm), \n",
    "            torch.sinh(x_norm) * x_unit\n",
    "        ], dim=1)\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def _depth_regularizer(self, x, batch):\n",
    "        with torch.no_grad():\n",
    "            rate = x * 0.2\n",
    "            samples = torch.poisson(rate)\n",
    "        \n",
    "        x_perturbed = F.relu(x - samples)\n",
    "        z_mu_hat, _ = self._encoder(x_perturbed, batch)\n",
    "        \n",
    "        mean_diff = torch.sum(torch.pow(self.z_mu - z_mu_hat, 2), dim=1)\n",
    "        loss = torch.mean(mean_diff)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def log_likelihood_nb(self, x, mu, sigma_square, eps=1e-10):\n",
    "        log_theta_mu_eps = torch.log(sigma_square + mu + eps)\n",
    "        res = (\n",
    "            torch.lgamma(x + sigma_square) - torch.lgamma(x + 1) - torch.lgamma(sigma_square) +\n",
    "            sigma_square * (torch.log(sigma_square) - log_theta_mu_eps) +\n",
    "            x * (torch.log(mu) - log_theta_mu_eps)\n",
    "        )\n",
    "        return res\n",
    "    \n",
    "    def log_likelihood_student(self, x, mu, sigma_square, df=5.0):\n",
    "        df_halved = df / 2\n",
    "        return (\n",
    "            torch.lgamma(df_halved + 0.5) - torch.lgamma(df_halved) -\n",
    "            0.5 * torch.log(math.pi * df * sigma_square) -\n",
    "            (df_halved + 0.5) * torch.log(1 + (x - mu)**2 / (df * sigma_square))\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, batch_id):\n",
    "        if len(self.n_batch) > 1:\n",
    "            batch = self.multi_one_hot(batch_id, self.n_batch)\n",
    "        else:\n",
    "            batch = F.one_hot(batch_id, self.n_batch[0]).float()\n",
    "        \n",
    "        self.library_size = torch.sum(x, dim=1, keepdim=True)\n",
    "        \n",
    "        self.z_mu, self.z_sigma_square = self._encoder(x, batch)\n",
    "        \n",
    "        # Sample latent variable\n",
    "        if self.latent_dist == \"normal\":\n",
    "            self.q_z = dist.Normal(self.z_mu, torch.sqrt(self.z_sigma_square))\n",
    "            self.z = self.q_z.rsample()\n",
    "            self.p_z = dist.Normal(torch.zeros_like(self.z), torch.ones_like(self.z))\n",
    "            kl = dist.kl_divergence(self.q_z, self.p_z).sum(dim=1)\n",
    "            self.kl = torch.mean(kl)\n",
    "        elif self.latent_dist == 'vmf':\n",
    "            self.q_z = VonMisesFisher(self.z_mu, self.z_sigma_square)\n",
    "            self.z = self.q_z.sample()\n",
    "            self.p_z = HypersphericalUniform(self.z_dim - 1, dtype=x.dtype)\n",
    "            kl = self.q_z.kl_divergence(self.p_z)\n",
    "            self.kl = torch.mean(kl)\n",
    "        elif self.latent_dist == 'wn':\n",
    "            self.q_z = HyperbolicWrappedNorm(self.z_mu, self.z_sigma_square)\n",
    "            self.z = self.q_z.sample()\n",
    "            tmp = self._polar_project(torch.zeros_like(self.z_sigma_square))\n",
    "            self.p_z = HyperbolicWrappedNorm(tmp, torch.ones_like(self.z_sigma_square))\n",
    "            kl = self.q_z.log_prob(self.z) - self.p_z.log_prob(self.z)\n",
    "            self.kl = torch.mean(kl)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        # Decoder\n",
    "        self.mu, self.sigma_square = self._decoder(self.z, batch)\n",
    "        \n",
    "        # Depth regularization\n",
    "        self.depth_loss = self._depth_regularizer(x, batch)\n",
    "        \n",
    "        # ELBO calculation\n",
    "        if"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mixed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
