{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/home/romainlhardy/code/hyperbolic-cancer/Mixed-Curvature-Pathways\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "from pytorch.hyperbolic_parameter import PoincareParameter, EuclideanParameter, SphericalParameter, HyperboloidParameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, dist_fn, param_cls, n, d, project=True, initialize=None, learn_scale=False, initial_scale=0.0):\n",
    "        super().__init__()\n",
    "        self.dist_fn = dist_fn # Distance function\n",
    "        self.n = n # Number of nodes\n",
    "        self.d = d # Dimension\n",
    "        self.project = project # Whether to project the embeddings\n",
    "        self.w = param_cls(data=initialize, sizes=(n, d)) # Embedding matrix\n",
    "        self.scale_log = nn.Parameter(torch.tensor([initial_scale], dtype=torch.double), requires_grad=learn_scale)\n",
    "\n",
    "    def scale(self):\n",
    "        scale = torch.exp(self.scale_log)\n",
    "        return scale\n",
    "\n",
    "    def dist_idx(self, idx):\n",
    "        wi = torch.index_select(self.w, 0, idx[:, 0])\n",
    "        wj = torch.index_select(self.w, 0, idx[:, 1])\n",
    "        d = self.dist_fn(wi, wj)\n",
    "        return d * self.scale()\n",
    "\n",
    "    def dist_row(self, i):\n",
    "        m = self.w.size(0)\n",
    "        return self.dist_fn(self.w[i, :].clone().unsqueeze(0).repeat(m, 1), self.w) * self.scale()\n",
    "\n",
    "    def dist_matrix(self):\n",
    "        m = self.w.size(0)\n",
    "        rets = torch.zeros(m, m, dtype=torch.double)\n",
    "        for i in range(m):\n",
    "            rets[i, :] = self.dist_row(i)\n",
    "        return rets\n",
    "\n",
    "    def normalize(self):\n",
    "        self.w.proj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acosh(x):\n",
    "    \"\"\"Inverse hyperbolic cosine.\"\"\"\n",
    "    return torch.log(x + torch.sqrt(x ** 2 - 1))\n",
    "\n",
    "def dot_h(x, y):\n",
    "    \"\"\"Inner product in hyperbolic space.\"\"\"\n",
    "    return torch.sum(x * y, -1) - 2 * x[..., 0] * y[..., 0]\n",
    "\n",
    "def dist_h(x, y):\n",
    "    \"\"\"Distance in hyperbolic space.\"\"\"\n",
    "    return acosh(torch.clamp(-dot_h(x, y), min=1.0 + 1e-8))\n",
    "\n",
    "def dist_e(u, v):\n",
    "    \"\"\"Distance in Euclidean space.\"\"\"\n",
    "    return torch.norm(u - v, 2, dim=1)\n",
    "\n",
    "def dist_s(u, v, eps=1e-9):\n",
    "    \"\"\"Distance in spherical space.\"\"\"\n",
    "    uu = SphericalParameter._proj(u)\n",
    "    vv = SphericalParameter._proj(v)\n",
    "    return torch.acos(torch.clamp(dot(uu, vv), -1+eps, 1-eps))\n",
    "\n",
    "class ProductEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_nodes, \n",
    "        h_dim, \n",
    "        h_copies=1, \n",
    "        e_dim=1, \n",
    "        e_copies=0, \n",
    "        s_dim=1, \n",
    "        s_copies=0, \n",
    "        project=True, \n",
    "        initialize=None, \n",
    "        learn_scale=False, \n",
    "        initial_scale=0.0, \n",
    "        absolute_loss=False, \n",
    "        logrel_loss=False, \n",
    "        dist_loss=False, \n",
    "        square_loss=False, \n",
    "        sym_loss=False, \n",
    "        exponential_rescale=None, \n",
    "        riemann=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.riemann = riemann\n",
    "\n",
    "        self.H = nn.ModuleList([Embedding(dist_h, HyperboloidParameter, num_nodes, h_dim, project, initialize, learn_scale, initial_scale) for _ in range(h_copies)])\n",
    "        self.E = nn.ModuleList([Embedding(dist_e, EuclideanParameter, num_nodes, e_dim, False, initialize, False, initial_scale) for _ in range(e_copies)])\n",
    "        self.S = nn.ModuleList([Embedding(dist_s, SphericalParameter, num_nodes, s_dim, project, initialize, learn_scale, initial_scale) for _ in range(s_copies)])\n",
    "\n",
    "        self.scale_params = [H.scale_log for H in self.H] + [E.scale_log for E in self.E] + [S.scale_log for S in self.S] if learn_scale else []\n",
    "        self.hyp_params = [H.w for H in self.H]\n",
    "        self.euc_params = [E.w for E in self.E]\n",
    "        self.sph_params = [S.w for S in self.S]\n",
    "        self.embed_params = [H.w for H in self.H] + [E.w for E in self.E] + [S.w for S in self.S]\n",
    "\n",
    "        self.absolute_loss = absolute_loss\n",
    "        self.logrel_loss = logrel_loss\n",
    "        self.dist_loss = dist_loss\n",
    "        self.square_loss = square_loss\n",
    "        self.sym_loss = sym_loss\n",
    "\n",
    "        self.exponential_rescale = exponential_rescale\n",
    "\n",
    "    def all_attr(self, fn):\n",
    "        H_attr = [fn(H) for H in self.H]\n",
    "        E_attr = [fn(E) for E in self.E]\n",
    "        S_attr = [fn(S) for S in self.S]\n",
    "        return H_attr + E_attr + S_attr\n",
    "\n",
    "    def embedding(self):\n",
    "        return torch.cat(self.all_attr(lambda emb: emb.w.view(-1)))\n",
    "\n",
    "    def scale(self):\n",
    "        return self.all_attr(lambda emb: emb.scale())\n",
    "\n",
    "    def dist_idx(self, idx):\n",
    "        d = self.all_attr(lambda emb: emb.dist_idx(idx))\n",
    "        if self.riemann:\n",
    "            return torch.norm(torch.stack(d, 0), 2, dim=0)\n",
    "        else:\n",
    "            return sum(d)\n",
    "\n",
    "    def dist_row(self, i):\n",
    "        d = self.all_attr(lambda emb: emb.dist_row(i))\n",
    "        if self.riemann:\n",
    "            return torch.norm(torch.stack(d, 0), 2, dim=0)\n",
    "        else:\n",
    "            return sum(d)\n",
    "\n",
    "    def dist_matrix(self):\n",
    "        d = self.all_attr(lambda emb: emb.dist_matrix())\n",
    "        if self.riemann:\n",
    "            return torch.norm(torch.stack(d), 2, dim=0)\n",
    "        else:\n",
    "            return sum(d)\n",
    "\n",
    "    def loss(self, _x):\n",
    "        idx, values, w = _x\n",
    "        d = self.dist_idx(idx)\n",
    "\n",
    "        term_rescale = w\n",
    "\n",
    "        if self.absolute_loss:\n",
    "            loss = torch.sum(term_rescale*(d - values)**2)\n",
    "        elif self.logrel_loss:\n",
    "            loss = torch.sum(torch.log((d/values)**2)**2)\n",
    "        elif self.dist_loss:\n",
    "            loss = torch.sum(torch.abs(term_rescale*((d/values) - 1)))\n",
    "        elif self.square_loss:\n",
    "            loss = torch.sum(term_rescale*torch.abs((d/values)**2 - 1))\n",
    "        else:\n",
    "            l1 = torch.sum(term_rescale*((d/values) - 1)**2)\n",
    "            l2 = torch.sum(term_rescale*((values/d) - 1)**2) if self.sym_loss else 0\n",
    "            loss = l1 + l2\n",
    "        return loss / values.size(0)\n",
    "\n",
    "    def normalize(self):\n",
    "        for H in self.H:\n",
    "            H.normalize()\n",
    "        for S in self.S:\n",
    "            S.normalize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poincare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
